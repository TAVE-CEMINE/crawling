# 라이브러리 import
#%pip install selenium
#%pip install webdriver_manager
#%pip install lxml
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from urllib.parse import quote_plus
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from lxml import html
import requests
import time
import csv
import re
import random

# 해시태그 검색하여 url 변수에 저장
def Search_hashtag(word):
    url = "https://www.instagram.com/explore/tags/" + str(word)
    return url

# 열린 페이지에서 첫 번째 게시물 클릭
def select_first(driver):
  first = driver.find_element(By.CSS_SELECTOR, "div._aagw")
  first.click()
  time.sleep(3)

# 첫 게시물 클릭 후 다음 게시물 클릭
def move_next(driver):
    right = driver.find_element(By.CSS_SELECTOR, "div._aaqg._aaqh")
    right.click()
    time.sleep(3)

# 크롤링 함수
# 함수 정의: 본문 내용, 해시태그, 작성일자, 링크 가져오기
def collect_links(driver):
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    links = soup.select('a[href^="/p/"]')
    return {'https://www.instagram.com' + link['href'] for link in links}

def get_content(driver):
    html = driver.page_source
    soup = BeautifulSoup(html, 'lxml')
    # 본문 내용
    try:
        content = soup.select('div._a9zs')[0].text
    except IndexError:
        content = ''
    # 해시태그
    tags = re.findall(r'#[^\s#,\\]+', content)
    
    # 작성일자
    try:
        date = soup.select('time.x1p4m5qa')[0]['datetime'][:10]
    except (IndexError, KeyError):
        date = ''
    
    # 링크
    link = driver.current_url
    
    data = [content, date, tags, link]
    return data

# 크롤링 시작
# 크롬 브라우저 열기
service = Service(ChromeDriverManager().install()) # 크롬드라이버 최신 버전 자동 설치
options = webdriver.ChromeOptions() # 크롬브라우저 옵션 설정
driver = webdriver.Chrome(service=service, options=options) # 크롬브라우저 제어
driver.implicitly_wait(10) # 페이지 로드까지 기다리는 시간, 초과하면 NoSuchElementException
driver.get('https://www.instagram.com')
time.sleep(3)

#인스타그램 로그인을 위한 계정 정보
username_input = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, "input[name='username']")))
username_input.send_keys('')

# 비밀번호 입력란에 입력
password_input = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, "input[name='password']")))
password_input.send_keys('')

# 로그인 버튼 클릭
login_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, "button[type='submit']")))
login_button.click()

time.sleep(5)

# 해시태그 검색
word = input("검색할 해시태그를 입력하세요: ")
word = str(word)
url = Search_hashtag(word)

# 검색 결과 페이지 열기
driver.get(url)
time.sleep(10)

# 첫 번째 게시물 클릭
select_first(driver)

# 데이터 수집 시작
results = []
# 수집할 게시물의 수
target = 5
for i in range(target):
    try:
        data = get_content(driver)
        results.append(data)
        move_next(driver)
    except NoSuchElementException:
        time.sleep(2)
        move_next(driver)
    time.sleep(5)
    
print(results[:2])

# 결과를 데이터프레임으로 저장
#%pip install --upgrade pandas
import pandas as pd
from datetime import datetime

date = datetime.today().strftime('%Y-%m-%d')

results_df = pd.DataFrame(results)

# 엑셀 파일로 저장
#%pip install openpyxl
results_df.columns = ['content', 'date', 'tags', 'link']
results_df.to_excel(date + '_about_' + word + '_insta_crawling.xlsx')

# WebDriver 종료
driver.quit()
